---
title: "R Notebook"
output: html_notebook
---

### Trying it on my own texts


Initially I thought to do some exploring through code since it seemed more efficient than searching through the meta-data myself

```{r}
library(gutenbergr)
library(dplyr)
library(tidytext)
library(tidyr)
library(stringr)
library(scales)
```
This chunk (below) is just to show where the information is downloaded. Once I figure out how to search the dataframe, you all are doomed. 
```{r}
gutenberg_metadata
```

I still haven't gotten use to calling the functions in R so I looked through the variable  and clicked on it within the top right corner of R Studio and looked through that way. I'll list the steps down so that I can come back and see if I can replicate this in code.

1. Search for Lewis Carroll texts, find author ID (7)
2. Search for Frank L. Baum texts, find author ID (42)
3. Search for A. A. Milne texts, find author ID (730)
```{r}
# I was hoping this line just downloaded the texts corresponding to the specific author IDs
gutenberg_download(730, mirror = "http://gutenberg.pglaf.org/")
#i was wrong
```
This chunk (below) lists the works under each author and stores it into the respective variable 
```{r}
lewis <- gutenberg_metadata %>% filter(author == "Carroll, Lewis")
baum <- gutenberg_metadata %>% filter(author == "Baum, L. Frank (Lyman Frank)")
milne <- gutenberg_metadata %>% filter(author == "Milne, A. A. (Alan Alexander)")
```
This chunk (below) adds the texts from each author into their respective variable
```{r}
carroll <- gutenberg_works(author == "Carroll, Lewis") %>%
  gutenberg_download(meta_fields = "title")
fbaum <- gutenberg_works(author == "Baum, L. Frank (Lyman Frank)") %>%
  gutenberg_download(meta_fields = "title")
amilne <- gutenberg_works(author == "Milne, A. A. (Alan Alexander)") %>%
  gutenberg_download(meta_fields = "title")
```
If you've noticed an error in downloading files from gutenberg, you may have to specify a mirror for each of the gutenberg_download() functions because the current default may not be specified. 

To check the default mirror (if answer is NA, then you have to specify. Try http://aleph.gutenberg.org)
```{r}
gutenberg_get_mirror()
```

Let me list (here) each process that needs to be done for the individual corpus
1. unnest_tokens(): make text into list of words
2. Tidy the works (perhaps before we unnest)
    - but this might just be for that specific library janeaustenr
3. data(stop_words), anti_join(stop_words): removes stop words from list 
4. count(word, sort = TRUE) : count words to store in list
5. plot count (one corpus)
6. Repeat 1-5 for the other two corpus
7. check plots for the other corpus and customize stop words (if needed)
8. ggplot() to compare alignment in word frequency
9. 2 correlation tests

```{r}
# unnest_tokens(): make text into list of words
lewis_books <- carroll %>%
  group_by(title) %>%
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]",
                                                 ignore_case = TRUE)))) %>%   ungroup()
```

```{r}
carrolltxts <- lewis_books %>%
  unnest_tokens(word, text)
```
2. _Tidy the works (perhaps before we unnest)_
     - but this might just be for that specific library janeaustenr
This sounds simpler than it seems, since it really takes more work (janeausten package had it easy)
2.1 Check how much of the title page needs to go (all of it, none of it, etc) 
2.2 are chapter titles still included or is it something that needs to be removed. Also how do we remove this without removing a mention of chapter within the novel that is pertinent to the text. 
```{r}
# data(stop_words), anti_join(stop_words): removes stop words from list 
data(stop_words)

carroll_tidy <- carrolltxts %>%
  anti_join(stop_words)
```
```{r}
# count(word, sort = TRUE) : count words to store in list
carroll_tidy %>%
  count(word, sort = TRUE)

```
By checking the list of the most frequent words, there is a slight moment of
```{r}
lewis_books %>% 
  filter(str_detect(text, "1")) %>% 
  select(text)
```



```{r}
# plot count (one corpus)
```


Including stop words (one corpus)
1. unnest tokens
2. count words
3. combine book-words and total words in a df
4. map word freq by plotting n/total by count
5. calculate freq by rank (rank = rownumber, term_freq = n/total)
6. plot freq by rank
    - best fit line
7. bind_tf_idf(word, book, n) <-calculates t, idf, and tf-idf
    - remove total word column and descending values of tf-idf
8. plot word vs tf_idf and separate by text 

Multiple Corpus
1. bind_tf_idf() for multiple corpus
2. plot word vs tf_idf (separately plotted by author)
3. customize stop_words by searching lines with the top words that are weird
4. re-plot word vs tf_idf (separately plotted by author)
```{r}

```

